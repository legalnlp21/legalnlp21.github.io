<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Legal - NLP</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
            <div class="container px-4">
                <a class="navbar-brand" href="#page-top">Legal-NLP</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#Know More">Know More</a></li>
                        <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Header-->
        <header class="bg-primary bg-gradient text-white">
            <div class="container px-4 text-center">
                <h1 class="fw-bolder">Welcome to LegalNLP </h1>
                <p class="lead">- Natural Language Processing Methods for the Brazilian Legal Language</p>
                <a class="btn btn-lg btn-light" href="#about">Know more</a>
            </div>
        </header>
        <!-- About section-->
        <section id="about">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <p>If you use our library in your academic work, please cite us in the following way</p>

<hr />

<h2 id="summary">Summary</h2>

<ol>
  <li><a href="#0">Accessing the Language Models</a></li>
  <li><a href="#1"> Introduction </a></li>
  <li><a href="#2">Fuctions </a>
    <ol>
      <li><a href="#2.1"> Text Cleaning Functions</a></li>
      <li><a href="#2.2">Other Functions</a></li>
    </ol>
  </li>
  <li><a href="#3"> Language Models </a>
    <ol>
      <li><a href="#3.1"> Phraser </a></li>
      <li><a href="#3.2"> Word2Vec/Doc2Vec </a></li>
      <li><a href="#3.3"> FastText </a></li>
      <li><a href="#3.4"> BERTikal </a></li>
    </ol>
  </li>
  <li><a href="#4"> Demonstrations </a></li>
  <li><a href="#5"> References</a></li>
</ol>

<hr />

<p><a name="0"></a></p>
<h2 id="0-address-for-language-models">0. Address for Language Models</h2>

<p>All our models can be found <a href="https://drive.google.com/drive/folders/1tCccOXPLSEAEUQtcWXvED3YaNJi3p7la?usp=sharing">here</a>.</p>

<p>Some models can be download directly using our function <code class="language-plaintext highlighter-rouge">get_premodel</code>.</p>

<p>Please contact (…) if you have some problem accessing the language models.</p>

<hr />

<p><a name="1"></a></p>
<h2 id="1-introduction">1. Introduction</h2>
<p><em>LegalNLP</em> is promising given the scarcity of Natural Language Processing resources focused on the Brazilian legal language. It is worth mentioning that our library was made for Python, one of the most well-known programming languages for machine learning.</p>

<p>You can access our paper by clicking <strong>here</strong>.</p>

<hr />

<p><a name="2"></a></p>
<h2 id="2-functions">2. Functions</h2>
<p><a name="2.1"></a></p>
<h3 id="21--text-cleaning-functions">2.1.  Text Cleaning Functions</h3>

<p><a name="2.1.1"></a></p>
<h4 id="211-cleantext-lowertrue-return_maskedfalse">2.1.1. <code class="language-plaintext highlighter-rouge">clean(text, lower=True, return_masked=False)</code></h4>
<p>Function for cleaning texts to be used (optional) in conjunction with Doc2Vec, Word2Vec, and FastText models. We use RegEx to mask/extract information such as email addresses, URLs, dates, numbers, monetary values, etc.</p>

<p><strong>input</strong>:</p>

<ul>
  <li>
    <p><em>text</em>, <strong>str</strong>;</p>
  </li>
  <li>
    <p><em>lower</em>, <strong>bool</strong>, default=<strong>True</strong>. If lower==True, function lower cases the whole text. Note that all the models (except BERT) were trained with lower cased texts;</p>
  </li>
  <li>
    <p><em>return_masked</em>, <strong>bool</strong>, default=<strong>True</strong>.  If return_masked == False, the function outputs a clean text. Otherwise, it returns a dictionary containing the clean text and the information extracted by RegEx;</p>
  </li>
</ul>

<p><strong>output</strong>:</p>

<ul>
  <li>Clean text or dictionary, depending on the <em>return_masked</em> parameter;</li>
</ul>

<p><a name="2.1.2"></a></p>
<h4 id="212clean_berttext">2.1.2.<code class="language-plaintext highlighter-rouge">clean_bert(text)</code></h4>

<p>Function for cleaning the texts to be used (optional) in conjunction with the BERT model.</p>

<p><strong>input:</strong></p>

<ul>
  <li><em>text</em>, <strong>str</strong>.</li>
</ul>

<p><strong>output:</strong></p>

<ul>
  <li><strong>str</strong> with clean text.</li>
</ul>

<p><a name="2.2"></a></p>
<h3 id="22--other-functions">2.2.  Other functions</h3>

<h4 id="222-get_premodelmodel">2.2.2. <code class="language-plaintext highlighter-rouge">get_premodel(model)</code></h4>

<p>Function to download a pre-trained model in the same folder as the file that is being executed.</p>

<p><strong>input:</strong></p>

<ul>
  <li><em>model</em>, <strong>str</strong>. Must contain the name of the pre-trained model that one wants to use. There are these options:
    <ul>
      <li><strong>model = “bert”</strong>: Download a .zip file containing BERTikal model and unzip it.</li>
      <li><strong>model = “wdoc</strong>: Download Word2Vec and Do2vec pre-trained models in a.zip file and unzip it. It has 2 two files, one with an size 100 Doc2Vec Distributed Memory/ Word2Vec Continuous Bag-of-Words (CBOW) embeddings generator and other with an size 100 Doc2Vec Distributed Bag-of-Words (DBOW)/ Word2Vec Skip-Gram (SG)  embeddings generator.</li>
      <li><strong>model = “phraser”</strong>: Download Phraser pre-trained model in a .zip file and unzip it. It has 2 two files with phraser1 and phreaser2 that will be explained how to use them in 3.1 section.</li>
      <li><strong>model = “w2vnilc”</strong>: Download size 100 Word2Vec CBOW model made by “Núcleo Interinstitucional de Linguística Computacional” embeddings generator in a .zip file and unzip it. More about in <a href="http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc">http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc</a>.</li>
      <li><strong>model = “neuralmindbase”</strong>: Download a .zip file containing base BERT model ,ade by NeuralMind and unzip it.</li>
      <li><strong>model = “neuralmindbase”</strong>: Download a .zip file containing base BERT model ,ade by NeuralMind and unzip it.</li>
      <li><strong>model = “neuralmindlarge”</strong>: Download a .zip file containing large BERT model ,ade by NeuralMind and unzip it. For more informations about BERT models made by NeuralMind go to <a href="https://github.com/neuralmind-ai/portuguese-bert">https://github.com/neuralmind-ai/portuguese-bert</a>.
<strong>output:</strong></li>
    </ul>
  </li>
  <li>True if download of some model was made and False otherwise.</li>
</ul>

<h4 id="221-extract_features_bertpath_model-path_tokenizer-data-gputrue">2.2.1. <code class="language-plaintext highlighter-rouge">extract_features_bert(path_model, path_tokenizer, data, gpu=True)</code></h4>

<p>Function for extracting features with the BERT model (This function is not accessed through the package installation, but you can find it <a href="https://github.com/legalnlp21/legalnlp/blob/main/demo/BERT/extract_features_bert.ipynb">here</a>).</p>

<p><strong>Input:</strong></p>

<ul>
  <li>
    <p><em>path_model</em>, <strong>str</strong>. Must contain the path of the pre-trained model;</p>
  </li>
  <li>
    <p><em>path_tokenizer</em>, <strong>str</strong>. Must contain the path of tokenizer;</p>
  </li>
  <li>
    <p><em>data</em>, <strong>list</strong>. Must contain a list of texts that will be extracted features;</p>
  </li>
  <li>
    <p><em>gpu</em>, <strong>bool</strong>, default=<strong>True</strong>. If gpu==False, the GPU will not be used in the model application (we recommend feature extraction to be done using Google Colab).</p>
  </li>
</ul>

<p><strong>Output:</strong></p>

<ul>
  <li><strong>DataFrame</strong> with features extracted by BERT model.</li>
</ul>

<p><a name="3"></a></p>
<h2 id="3-model-languages">3. Model Languages</h2>

<p><a name="3.1"></a></p>
<h3 id="31-phraser">3.1. Phraser</h3>

<p>Phraser is a statistical method proposed in the natural language processing
literature [1] for identifying which words when they appear
together, can be considered as unique tokens. This method application is able to
identify the relevance of the occurrence of a bigram against the occurrence of the
words that make it up separately. Thus, we can identify that a bigram like “São
Paulo” should be treated as a single token, for example. If the method is applied
a second time in sequence, we can check which are the relevant trigrams and
quadrigrams. Since the two applications should be done with different Phraser
models, it can be the case that the second application identifies bigrams that were
not identified by the first model.</p>

<p>This model is compatible with the <code class="language-plaintext highlighter-rouge">clean</code> function, but it is not necessary to use it before. Remember to at least make all letters lowercase. Please check our paper or <a href="https://radimrehurek.com/gensim_3.8.3/models/phrases.html">Gensim page</a> for more details. Preferably use Gensim version 3.8.3.</p>

<h4 id="using-phraser">Using <em>Phraser</em></h4>
<p>Installing Gensim</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="s">'3.8.3'</span> 
</code></pre></div></div>

<p>Importing package and loading our two Phraser models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Importing packages
</span><span class="kn">from</span> <span class="nn">gensim.models.phrases</span> <span class="kn">import</span> <span class="n">Phraser</span> 

<span class="c1">#Loading two Phraser models
</span><span class="n">phraser1</span><span class="o">=</span><span class="n">Phraser</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'models_phraser/phraser1'</span><span class="p">)</span>
<span class="n">phraser2</span><span class="o">=</span><span class="n">Phraser</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'models_phraser/phraser2'</span><span class="p">)</span>
</code></pre></div></div>

<p>Applying Phraser once and twice to check output</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">txt</span><span class="o">=</span><span class="s">'direito do consumidor origem : bangu regional xxix juizado especial civel ação : [processo] - - recte : fundo de investimento em direitos creditórios'</span>
<span class="n">tokens</span><span class="o">=</span><span class="n">txt</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Clean Text: "'</span><span class="o">+</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">+</span><span class="s">'"'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Applying Phraser 1x: "'</span><span class="o">+</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">phraser1</span><span class="p">[</span><span class="n">tokens</span><span class="p">])</span><span class="o">+</span><span class="s">'"'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Applying Phraser 2x: "'</span><span class="o">+</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">phraser2</span><span class="p">[</span><span class="n">phraser1</span><span class="p">[</span><span class="n">tokens</span><span class="p">]])</span><span class="o">+</span><span class="s">'"'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Clean Text: "direito do consumidor origem : bangu regional xxix juizado especial civel ação : [processo] - - recte : fundo de investimento em direitos creditórios"

Applying Phraser 1x: "direito do consumidor origem : bangu regional xxix juizado_especial civel_ação : [processo] - - recte : fundo de investimento em direitos_creditórios"

Applying Phraser 2x: "direito do consumidor origem : bangu_regional xxix juizado_especial_civel_ação : [processo] - - recte : fundo de investimento em direitos_creditórios"
</code></pre></div></div>

<p><a name="3.2"></a></p>
<h3 id="32-word2vecdoc2vec">3.2. Word2Vec/Doc2Vec</h3>

<p>Our first models for generating vector representation for tokens and
texts (embeddings) are variations of the Word2Vec [1,
2] and Doc2Vec [3] methods. In short, the
Word2Vec methods generate embeddings for tokens5 and that somehow capture
the meaning of the various textual elements, based on the contexts in which these
elements appear. Doc2Vec methods are extensions/modifications of Word2Vec
for generating whole text representations.</p>

<p>The Word2Vec and Doc2Vec methods are presented together in this section because they were trained together using the Gensim package. Both models are compatible with the <code class="language-plaintext highlighter-rouge">clean</code> function, but it is not necessary to use it before. Remember to at least make all letters lowercase. Please check our paper or <a href="https://radimrehurek.com/gensim_3.8.3/models/doc2vec.html">Gensim page</a> for more details. Preferably use Gensim version 3.8.3.</p>

<p>Below we have a summary table with some important information about the trained models:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Filenames</th>
      <th style="text-align: center">Doc2Vec</th>
      <th style="text-align: center">Word2Vec</th>
      <th style="text-align: center">Size</th>
      <th style="text-align: center">Windows</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">w2v_d2v_dm*</code></td>
      <td style="text-align: center">Distributed Memory       (DM)</td>
      <td style="text-align: center">Continuous Bag-of-Words (CBOW)</td>
      <td style="text-align: center">100, 200, 300</td>
      <td style="text-align: center">15</td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">w2v_d2v_dbow*</code></td>
      <td style="text-align: center">Distributed Bag-of-Words (DBOW)</td>
      <td style="text-align: center">Skip-Gram (SG)</td>
      <td style="text-align: center">100, 200, 300</td>
      <td style="text-align: center">15</td>
    </tr>
  </tbody>
</table>

<h4 id="using-word2vec">Using <em>Word2Vec</em></h4>

<p>Installing Gensim</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="s">'3.8.3'</span> 
</code></pre></div></div>

<p>Loading W2V (all the files for the specific model should be in the same folder)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#Loading a W2V model
</span><span class="n">w2v</span><span class="o">=</span><span class="n">KeyedVectors</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'models_w2v_d2v/w2v_d2v_dm_size_100_window_15_epochs_20'</span><span class="p">)</span>
<span class="n">w2v</span><span class="o">=</span><span class="n">w2v</span><span class="p">.</span><span class="n">wv</span>
</code></pre></div></div>
<p>Viewing the first 10 entries of ‘juiz’ vector</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w2v</span><span class="p">[</span><span class="s">'juiz'</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 6.570131  , -1.262787  ,  5.156106  , -8.943866  , -5.884408  ,
       -7.717058  ,  1.8819941 , -8.02803   , -0.66901577,  6.7223144 ],
      dtype=float32)
</code></pre></div></div>

<p>Viewing closest tokens to ‘juiz’</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w2v</span><span class="p">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'juiz'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('juíza', 0.8210258483886719),
 ('juiza', 0.7306275367736816),
 ('juíz', 0.691645085811615),
 ('juízo', 0.6605231165885925),
 ('magistrado', 0.6213295459747314),
 ('mmª_juíza', 0.5510469675064087),
 ('juizo', 0.5494943261146545),
 ('desembargador', 0.5313084721565247),
 ('mmjuiz', 0.5277603268623352),
 ('fabíola_melo_feijão_juíza', 0.5043971538543701)]
</code></pre></div></div>

<h4 id="using-doc2vec">Using <em>Doc2Vec</em></h4>
<p>Installing Gensim</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="s">'3.8.3'</span> 
</code></pre></div></div>

<p>Loading D2V (all the files for the specific model should be in the same folder)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Doc2Vec</span>

<span class="c1">#Loading a D2V model
</span><span class="n">d2v</span><span class="o">=</span><span class="n">Doc2Vec</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'models_w2v_d2v/w2v_d2v_dm_size_100_window_15_epochs_20'</span><span class="p">)</span>
</code></pre></div></div>

<p>Inferring vector for a text</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">txt</span><span class="o">=</span><span class="s">'direito do consumidor origem : bangu regional xxix juizado especial civel ação : [processo] - - recte : fundo de investimento em direitos creditórios'</span>
<span class="n">tokens</span><span class="o">=</span><span class="n">txt</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>

<span class="n">txt_vec</span><span class="o">=</span><span class="n">d2v</span><span class="p">.</span><span class="n">infer_vector</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">txt_vec</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 0.02626514, -0.3876521 , -0.24873355, -0.0318402 ,  0.3343679 ,
       -0.21307918,  0.07193747,  0.02030687,  0.407305  ,  0.20065512],
      dtype=float32)
</code></pre></div></div>

<p><a name="3.3"></a></p>
<h3 id="33-fasttext">3.3. FastText</h3>

<p>The FastText [4] methods, like Word2Vec, form a class of
models for creating vector representations (embeddings) for tokens. Unlike
Word2Vec, which disregards the morphology of the tokens and allocates a
different vector for each one of them, the FastText methods consider that each one
of the tokens is formed by n-grams of characters or substrings. In this way, the
representation of tokens which do not appear in the training set can be inferred
from the representation of substrings. Also, rare tokens can have more robust
representations than those returned by the Word2Vec methods.</p>

<p>Models are compatible with the <code class="language-plaintext highlighter-rouge">clean</code> function, but it is not necessary to use it. Remember to at least make all letters lowercase. Please check our paper or the <a href="https://radimrehurek.com/gensim/models/fasttext.html">Gensim page</a> for more details. Preferably use Gensim version 4.0.1.</p>

<p>Below we have a summary table with some important information about the trained models:
| Filenames      | FastText   | Sizes | Windows
|:——————-:|:————–:|:————–:|:————–:|
| <code class="language-plaintext highlighter-rouge">fasttext_cbow*</code>         | Continuous Bag-of-Words (CBOW)          | 100, 200, 300 | 15 
| <code class="language-plaintext highlighter-rouge">fasttext_sg*</code>             | Skip-Gram (SG)                   | 100, 200, 300      | 15</p>

<h4 id="using-fasttext">Using <em>FastText</em></h4>

<p>installing Gensim</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="s">'4.0.1'</span> 
</code></pre></div></div>

<p>Loading FastText (all the files for the specific model should be in the same folder)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">FastText</span>

<span class="c1">#Loading a FastText model
</span><span class="n">fast</span><span class="o">=</span><span class="n">FastText</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'models_fasttext/fasttext_sg_size_100_window_15_epochs_20'</span><span class="p">)</span>
<span class="n">fast</span><span class="o">=</span><span class="n">fast</span><span class="p">.</span><span class="n">wv</span>
</code></pre></div></div>

<p>Viewing the first 10 entries of ‘juiz’ vector</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fast</span><span class="p">[</span><span class="s">'juiz'</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 0.46769685,  0.62529474,  0.08549586,  0.09621219, -0.09998254,
       -0.07897531,  0.32838237, -0.33229044, -0.05959201, -0.5865443 ],
      dtype=float32)
</code></pre></div></div>

<p>Viewing the first 10 vector entries of a token that was not in our vocabulary</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fast</span><span class="p">[</span><span class="s">'juizasjashdkjhaskda'</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 0.02795791,  0.1361525 ,  0.1340836 , -0.36824217, -0.11549155,
       -0.11167661,  0.32045627, -0.33701468, -0.05198409, -0.05513595],
      dtype=float32)
</code></pre></div></div>

<p><a name="3.4"></a></p>
<h3 id="34-bertikal">3.4. BERTikal</h3>

<p>We call BERTikal our BERT-Base model   (cased) [5] for Brazilian legal language. BERT models are models based on neural network architectures called Transformers. BERT models are trained with large sets of texts using the self-supervised paradigm, which is basically solving unsupervised problems using supervised techniques. A pre-trained BERT model is capable of generating representations for entire texts and can be adapted for a supervised task, e.g., text classification or question answering, using the fine-tuning mechanism.</p>

<p>BERTikal was trained using the Python package <a href="https://huggingface.co/transformers/}">Transformers</a>  in its 4.2.2 version and its checkpoint made available by us is compatible with <a href="https://pytorch.org/">PyTorch</a> 1.9.0. Although we expose the versions of both packages, more current versions can be used in applications of the model, as long as there are no relevant version conflicts.</p>

<p>Our model was trained from the checkpoint made available in <a href="https://github.com/neuralmind-ai/portuguese-bert">Neuralmind’s Github repository</a> by the authors of recent research [6].</p>

<h4 id="using-bertikal">Using <em>BERTikal</em></h4>

<p>Installing Torch e Transformers</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span><span class="o">==</span><span class="s">'1.8.1'</span> <span class="n">transformers</span><span class="o">==</span><span class="s">'4.2.2'</span>
</code></pre></div></div>

<p>Loading BERT (all the files for the specific model should be in the same folder)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertTokenizer</span>

<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'model_bertikal/'</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">bert_model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'model_bertikal/'</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<p><a name="4"></a></p>
<h2 id="4-demonstrations">4. Demonstrations</h2>

<p>For a better understanding of the application of these models, below are the links to notebooks where we apply them to a legal dataset using various classification models such as Logistic Regression and CatBoost:</p>

<ul>
  <li><strong>BERT notebook</strong> : 
<a href="https://github.com/legalnlp21/legalnlp/blob/main/demo/BERT/BERT_TUTORIAL.ipynb">https://github.com/legalnlp21/legalnlp/blob/main/demo/BERT/BERT_TUTORIAL.ipynb</a></li>
  <li><strong>Word2Vec notebook</strong> :
<a href="https://github.com/legalnlp21/legalnlp/blob/main/demo/Word2Vec/Word2Vec_TUTORIAL.ipynb">https://github.com/legalnlp21/legalnlp/blob/main/demo/Word2Vec/Word2Vec_TUTORIAL.ipynb</a></li>
  <li><strong>Doc2Vec notebook</strong> :
<a href="https://github.com/legalnlp21/legalnlp/tree/main/demo/Doc2Vec">https://github.com/legalnlp21/legalnlp/tree/main/demo/Doc2Vec</a></li>
</ul>

<hr />

<p><a name="5"></a></p>
<h2 id="5-references">5. References</h2>

<p>[1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013b).
Distributed representations of words and phrases and their compositionality.
In Advances in neural information processing systems, pages 3111–3119.</p>

<p>[2] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of
word representations in vector space. arXiv preprint arXiv:1301.3781.</p>

<p>[3] Le, Q. and Mikolov, T. (2014). Distributed representations of sentences and
documents. In International conference on machine learning, pages 1188–1196.
PMLR.</p>

<p>[4] Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). Enriching
word vectors with subword information. Transactions of the Association for
Computational Linguistics, 5:135–146.</p>

<p>[5] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training
of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805.</p>

<p>[6] Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTimbau: pretrained BERT
models for Brazilian Portuguese. In 9th Brazilian Conference on Intelligent
Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23</p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Services section-->
        <section class="bg-light" id="Know More">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2>Github Repo</h2>
                        <p class="lead">You can see our work and contribute to it in: </p>
                        <ul>
                            <li><strong>Github</strong> : 
                          <a href="https://github.com/legalnlp21/legalnlp">https://github.com/legalnlp21/legalnlp</a></li>
                            <li><strong>Article</strong> : Waiting for publication
                        </ul>
                    </div>
                </div>
            </div>
        </section>
        <!-- Contact section-->
        <section id="contact">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2>Contact us</h2>
                        <p class="lead">Contact us via email: projetonlp2021@gmail.com.</p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Footer-->
        <footer class="py-5 bg-dark">
            <div class="container px-4"><p class="m-0 text-center text-white">Copyright &copy; Legal-NLP 2021</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
